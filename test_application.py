# -*- coding: utf-8 -*-
"""TestApplication.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WL7hrfvsl8Xiw85PK6kBycCS0EMcJUgH
"""

!git clone https://github.com/huggingface/transformers \
&& cd transformers \
&& git checkout a3085020ed0d81d4903c50967687192e3101e770 

!pip3 install ./transformers

!pip3 install -Uq openai wandb

!pip3 install sentencepiece

from transformers import pipeline
from transformers import BertForQuestionAnswering, AutoTokenizer

from transformers import (
    AlbertConfig,
    AlbertForQuestionAnswering,
    AlbertTokenizer,
    squad_convert_examples_to_features
)

import openai
import json
import pandas as pd

def bert_based_models(nlp, context, question):
  ans = nlp({
    'question': question,
    'context': context})
  return ans['answer']

def gpt_model(gpt_model,question,openAIkey):
  openai.api_key = openAIkey
  response = openai.Completion.create(
      model= gpt_model,
      prompt= question,
      temperature=0)
  # answer = !openai -k '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' api completions.create -m 'curie:ft-personal-2022-08-12-23-49-58' -p str(qstn)
  # repeat_to_get_max.append(answer[-1])

  answer = response['choices'][0]['text'].split('\n\n###\n\n ')
  if len(answer) == 0:
    answer = answer[0]
  else:
    answer = " ".join(answer[1:])  

  return answer

context = 'People with COVID-19 have had a wide range of symptoms reported â€“ ranging from mild symptoms to severe illness. Symptoms may appear 2-14 days after exposure to the virus. Anyone can have mild to severe symptoms.Possible symptoms include:Fever or chills, Cough, Shortness of breath or difficulty breathing,Fatigue, Muscle or body aches, Headache'

def main():
  print('Welcome to CovQuA Test Application!! A one-stop  application for all your COVID related questions.')
  print()
  context = input('Enter the context: \n')
  print()
  question = input('Enter the question: \t')

  print()
  # Calling ALBERT model instance
  print('Running ALBERT model')
  albert_modelname = 'twmkn9/albert-base-v2-squad2'
  albert_model = AlbertForQuestionAnswering.from_pretrained(albert_modelname)
  albert_tokenizer = AlbertTokenizer.from_pretrained(albert_modelname)

  nlp_albert = pipeline('question-answering', model=albert_model, tokenizer=albert_tokenizer, device=0)
  answer_albert = bert_based_models(nlp_albert, context, question)

  # Calling BERT model instance
  print()
  print('Running BERT model')
  bert_modelname = 'deepset/bert-base-cased-squad2'
  bert_model = BertForQuestionAnswering.from_pretrained(bert_modelname)
  bert_tokenizer = AutoTokenizer.from_pretrained(bert_modelname)

  nlp_bert = pipeline('question-answering', model=bert_model, tokenizer=bert_tokenizer, device=0)
  answer_bert = bert_based_models(nlp_bert, context, question)

  # Calling GPT model instance
  print()
  print('Running GPT model')
  openAIkey = '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'
  gpt_model_name = 'ada:ft-personal-2022-08-14-11-39-12'
  answer_gpt = gpt_model(gpt_model_name,question,openAIkey)

  print()
  print('The answers generated are as follows')
  print()
  print('ALBERT:\t', answer_albert)
  print('BERT:\t', answer_bert)
  print('GPT:\t', answer_gpt)

main()

# Running again to show another example (Improvevment to be done: Better Handling of I/O )
main()